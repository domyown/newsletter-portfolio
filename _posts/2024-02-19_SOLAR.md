**SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling**


1. Introduction

   * Previous Method's Limitation
     - For up-scaling, *Mixture-of-Experts (MoE)* require non-trivial changes to the training (e.g. gating network, dyamic expert selection) and inference framework (e.g. specialized CUDA kernels for fast inference), which hinders widespread applicability.
       
   * Proposal
     - Depth up-scaling (DUS) emcompasses depthwise scaling and comtinued pretraining, which is simple yet effective.
     - Using DUS, SOLAR 10.7B outperforms Llama 2 and Mistral 7B. 

2. Method : Depth up-scaling (DUS)
   * Base model
     - Architecture : 32-layer Llama 2
     - Initialized Pre-trained weights : Mistral 7B
   * Depthwise scaling
     - Step 1: Duplicate the base model ->
   ![image](https://github.com/domyown/newsletter-portfolio/assets/43026521/8d11ebd7-1509-4a4c-8c09-23dd424da5df)
   * Continued pretraining
     - Rapid performance recovery of the scaled model during continued pretraining is observed.

4. 

